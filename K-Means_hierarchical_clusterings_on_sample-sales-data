# --- Import Libraries ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

# --- Step 1: Load the Dataset ---
data = pd.read_csv("sales_data_sample.csv", encoding='latin1')

print("Dataset Preview:")
print(data.head())

print("\nDataset Info:")
print(data.info())

# --- Step 2: Select Relevant Numerical Features ---
# We'll use features related to sales and quantity for clustering
numerical_cols = ['QUANTITYORDERED', 'PRICEEACH', 'SALES']
df = data[numerical_cols].dropna()

print("\nSelected Columns for Clustering:")
print(df.head())

# --- Step 3: Normalize the Data ---
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)

# --- Step 4: Determine Optimal Number of Clusters using Elbow Method ---
inertia = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(df_scaled)
    inertia.append(kmeans.inertia_)  # Sum of squared distances to cluster centers

# Plot the Elbow curve
plt.figure(figsize=(7,4))
plt.plot(K_range, inertia, 'bo--')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia (Within-Cluster Sum of Squares)')
plt.title('Elbow Method to Determine Optimal K')
plt.grid(True)
plt.show()

# --- Step 5: Apply K-Means with Optimal K (from elbow, e.g., 3) ---
optimal_k = 3  # You can choose based on the elbow point in the graph
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(df_scaled)

print("\nCluster Centers (scaled values):")
print(kmeans.cluster_centers_)

print("\nCluster Distribution:")
print(df['Cluster'].value_counts())

# --- Step 6: Visualize the Clusters ---
plt.figure(figsize=(7,5))
sns.scatterplot(
    x=df['QUANTITYORDERED'],
    y=df['SALES'],
    hue=df['Cluster'],
    palette='viridis',
    s=60
)
plt.title('K-Means Clusters on Sales Data')
plt.xlabel('Quantity Ordered')
plt.ylabel('Sales')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()

# --- Optional Step 7: Hierarchical Clustering (for comparison) ---
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

linked = linkage(df_scaled, method='ward')

plt.figure(figsize=(8, 5))
dendrogram(linked, truncate_mode='lastp', p=10)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Cluster Index')
plt.ylabel('Distance')
plt.show()

# Choose number of clusters (same as K-Means)
labels = fcluster(linked, t=optimal_k, criterion='maxclust')
df['Hierarchical_Cluster'] = labels

print("\nHierarchical Cluster Distribution:")
print(df['Hierarchical_Cluster'].value_counts())
